# ==============================================================================
# Lunar - Predictive Recursive Descent Parser Generator
#
# Companion to Solar. Given the same grammar, Solar generates SLR(1) table
# parsers and Lunar generates predictive recursive descent parsers.
#
# Author: Steve Shreeve <steve.shreeve@gmail.com>
#   Date: February 13, 2026
# ==============================================================================

export install = (Generator) ->
  Generator::generateRD = ->
    infra   = @_generateRDInfra()
    parsers = @_generateRDParsers()
    shell   = @_generateRDShell()

    """
    // Predictive Recursive Descent Parser generated by Lunar

    #{infra}

    #{parsers}

    #{shell}
    """

  # --- Token management layer (static template) ---
  Generator::_generateRDInfra = ->
    """
    const ruleActions = #{@ruleActions};

    let token, tokenText, tokenLoc, lexerAdapter;
    const EOF = 1;

    function advance() {
      token = lexerAdapter.lex() || EOF;
      tokenText = lexerAdapter.text;
      tokenLoc = lexerAdapter.loc || {};
      return tokenText;
    }

    function expect(tag) {
      if (token !== tag) {
        const got = token === EOF ? 'end of input' : ("'" + token + "'");
        const line = (tokenLoc && tokenLoc.r || 0) + 1;
        throw new Error("Parse error on line " + line + ": expected '" + tag + "', got " + got);
      }
      const val = tokenText;
      advance();
      return val;
    }

    function match(tag) {
      if (token === tag) {
        const val = tokenText;
        advance();
        return val;
      }
    }

    function loc() {
      return tokenLoc ? { r: tokenLoc.r, c: tokenLoc.c } : { r: 0, c: 0 };
    }

    function withLoc(node, l) {
      if (Array.isArray(node)) node.loc = l || loc();
      return node;
    }

    function mark() {
      return { pos: lexerAdapter.pos, token, tokenText, tokenLoc: tokenLoc ? { r: tokenLoc.r, c: tokenLoc.c } : {} };
    }

    function reset(saved) {
      lexerAdapter.pos = saved.pos;
      token = saved.token;
      tokenText = saved.tokenText;
      tokenLoc = saved.tokenLoc;
    }

    function speculate(fn) {
      const saved = mark();
      try { return fn(); }
      catch (e) { reset(saved); return null; }
    }
    """

  # --- Parser shell and exports (static template) ---
  Generator::_generateRDShell = ->
    """
    const parserInstance = {
      parse(input) {
        lexerAdapter = Object.create(this.lexer);
        const sharedState = { ctx: {} };
        for (const k in this.ctx) sharedState.ctx[k] = this.ctx[k];
        lexerAdapter.setInput(input, sharedState.ctx);
        sharedState.ctx.lexer = lexerAdapter;
        sharedState.ctx.parser = this;
        if (!lexerAdapter.loc) lexerAdapter.loc = {};
        advance();
        return parseRoot();
      },
      lexer: null,
      ctx: {},
    };

    const createParser = (init = {}) => {
      const p = Object.create(parserInstance);
      Object.defineProperty(p, "ctx", {
        value: { ...init },
        enumerable: false,
        writable: true,
        configurable: true,
      });
      return p;
    };

    const parser = /*#__PURE__*/createParser();

    export { parser };
    export const Parser = createParser;
    export const parse = parser.parse.bind(parser);
    export default parser;
    """

  # --- All nonterminal parsing functions ---
  Generator::_generateRDParsers = ->
    lines = []

    # Build helper data structures
    @_buildFirstSets()

    # Analyze grammar for Pratt parser generation
    @_analyzeExpressionRules()

    # Generate Pratt binding power table and expression parsers
    lines.push @_generateRDBindingPowers()
    lines.push @_generateRDUnaryGeneric()
    lines.push @_generateRDPostfixForGeneric()

    # Generate each nonterminal parser
    # Some nonterminals need specialized implementations
    specialized = new Set ['For', 'Object', 'AssignObj']

    for own name of @types when name isnt '$accept'
      if specialized.has name
        lines.push @_generateRDSpecialized name
        continue

      category = @_classifyNonterminal name
      switch category
        when 'root'          then lines.push @_generateRDRoot()
        when 'body-list'     then lines.push @_generateRDBodyList name
        when 'comma-list'    then lines.push @_generateRDCommaList name
        when 'concat-list'   then lines.push @_generateRDConcatList name
        when 'expression'    then lines.push @_generateRDExpression()
        when 'expression-line' then lines.push @_generateRDExpressionLine()
        when 'operation'     then lines.push @_generateRDOperation()
        when 'operation-line' then lines.push @_generateRDOperationLine()
        when 'choice'        then lines.push @_generateRDChoice name
        when 'token'         then lines.push @_generateRDToken name
        when 'keyword'       then lines.push @_generateRDKeyword name
        when 'left-rec-loop' then lines.push @_generateRDLeftRecLoop name
        when 'sequence'      then lines.push @_generateRDSequence name
        else
          lines.push @_generateRDGeneric name

    lines.join '\n\n'

  # --- Build FIRST set lookup for terminals ---
  Generator::_buildFirstSets = ->
    @firsts = {}
    for own name, type of @types
      tokens = new Set
      type.firsts.forEach (t) => tokens.add t
      @firsts[name] = tokens

  # --- Classify a nonterminal by its grammar pattern (fully generic) ---
  Generator::_classifyNonterminal = (name) ->
    type = @types[name]
    return null unless type
    rules = type.rules

    # Start symbol → root
    return 'root' if name is @start

    # Known expression-related roles (detected by _analyzeExpressionRules)
    return 'expression'      if name is @_exprNT
    return 'expression-line' if name is @_exprLineNT
    return 'operation'       if name is @_operationNT
    return 'operation-line'  if name is @_operationLineNT

    # Left-recursive with TERMINATOR separator → body-list
    if rules.some((r) => r.symbols[0] is name and r.symbols[1] is 'TERMINATOR')
      return 'body-list'

    # Left-recursive with comma separator → comma-list
    if rules.some((r) => r.symbols[0] is name and r.symbols[1] is ',')
      return 'comma-list'

    # Left-recursive with no separator (concatenation) → concat-list
    hasConcatRec = rules.some((r) => r.symbols.length is 2 and r.symbols[0] is name and @types[r.symbols[1]])
    hasSepRec = rules.some((r) => r.symbols[0] is name and (r.symbols[1] is ',' or r.symbols[1] is 'TERMINATOR'))
    return 'concat-list' if hasConcatRec and not hasSepRec

    # Left-recursive with terminal continuation (e.g., IfBlock → IfBlock ELSE IF ...) → left-rec-loop
    hasLeftRec = rules.some((r) => r.symbols[0] is name and r.symbols.length >= 3 and not @types[r.symbols[1]])
    hasBase = rules.some((r) => r.symbols[0] isnt name)
    return 'left-rec-loop' if hasLeftRec and hasBase and not hasSepRec

    # Single rule with single token → token passthrough
    if rules.length is 1 and rules[0].symbols.length is 1 and not @types[rules[0].symbols[0]]
      return 'token'

    # All rules start with a unique keyword token → keyword-prefixed
    firstTokens = rules.map (r) => r.symbols[0]
    if firstTokens.every((s) => s and s isnt '' and not @types[s])
      uniqueFirsts = new Set firstTokens
      if uniqueFirsts.size is rules.length or firstTokens[0] is firstTokens[1]
        return 'keyword'

    # All rules are single-symbol nonterminal passthrough → choice
    if rules.every((r) => r.symbols.length is 1 and @types[r.symbols[0]])
      return 'choice'

    # Mixed choice — but check if a passthrough nonterminal is also the prefix
    # of a longer rule (e.g., ObjAssignable AND ObjAssignable : Expression).
    # If so, it's a shared-prefix sequence, not a choice.
    if rules.some((r) => r.symbols.length is 1 and @types[r.symbols[0]])
      passthroughs = (r.symbols[0] for r in rules when r.symbols.length is 1 and @types[r.symbols[0]])
      longerRules  = (r for r in rules when r.symbols.length > 1)
      hasSharedPrefix = passthroughs.some (nt) =>
        # Only count as shared prefix if the longer rule starts with the SAME nonterminal
        # AND none of the longer rules start with the expression nonterminal (which would cause infinite recursion)
        longerRules.some((r) => r.symbols[0] is nt) and not longerRules.some((r) => r.symbols[0] is @_exprNT or r.symbols[0] is @_operationNT)
      return 'sequence' if hasSharedPrefix
      return 'choice'

    'sequence'

  # --- Find the item nonterminal for a list pattern ---
  Generator::_findListItem = (listName, separator) ->
    type = @types[listName]
    return @_exprNT unless type

    # Look for the non-recursive single-symbol alternative: List → Item
    for rule in type.rules
      if rule.symbols.length is 1 and @types[rule.symbols[0]] and rule.symbols[0] isnt listName
        return rule.symbols[0]

    # Look for the item in the recursive rule: List → List separator Item
    for rule in type.rules
      syms = rule.symbols
      if syms[0] is listName and syms[1] is separator and syms.length >= 3
        lastSym = syms[syms.length - 1]
        return lastSym if @types[lastSym]

    @_exprNT  # fallback

  # ============================================================================
  # Generic Expression Analysis — derives Pratt handlers from grammar rules
  # ============================================================================

  Generator::_analyzeExpressionRules = ->
    # Detect nonterminal roles from grammar structure (no hardcoded names)
    @_exprNT = null           # The main expression nonterminal
    @_exprLineNT = null       # Single-line expression variant
    @_operationNT = null      # Nonterminal with binary operator rules
    @_operationLineNT = null  # Single-line operation variant
    @_valueNT = null          # Nonterminal with atom alternatives
    @_codeNT = null           # Arrow function nonterminal

    # Find the operation nonterminal — the one with the MOST binary operator
    # rules of the form NT OP NT where both operands are the same nonterminal
    # and OP has defined precedence (e.g., Expression + Expression)
    bestCount = 0
    for own name, type of @types
      binOpCount = 0
      for rule in type.rules
        syms = rule.symbols
        if syms.length >= 3 and @types[syms[0]] and @types[syms[2]] and syms[0] is syms[2] and not @types[syms[1]] and @operators[syms[1]]
          binOpCount++
      if binOpCount > bestCount
        bestCount = binOpCount
        @_operationNT = name

    # Find the expression nonterminal (has the operation NT as a single-symbol alternative)
    if @_operationNT
      for own name, type of @types when name isnt @_operationNT
        hasOperationAlt = type.rules.some (r) => r.symbols.length is 1 and r.symbols[0] is @_operationNT
        if hasOperationAlt
          @_exprNT ?= name

    # Find Value (atom container) and Code (arrow functions) from Expression alternatives
    if @_exprNT
      for rule in @types[@_exprNT].rules when rule.symbols.length is 1
        altName = rule.symbols[0]
        altType = @types[altName]
        continue unless altType
        continue if altName is @_operationNT
        # Value: pure choice nonterminal (all rules are single-nonterminal passthroughs)
        # with the most alternatives — this is the main "atom container"
        allPassthrough = altType.rules.every (r) => r.symbols.length is 1 and @types[r.symbols[0]]
        if allPassthrough and altType.rules.length >= 3
          if not @_valueNT or altType.rules.length > @types[@_valueNT].rules.length
            @_valueNT = altName
        # Code: FIRST set contains function glyphs (-> or =>)
        altFirsts = @firsts[altName]
        if altFirsts and (altFirsts.has('->') or altFirsts.has('=>'))
          @_codeNT ?= altName unless allPassthrough  # Code is a sequence, not a choice

    # Find expression-line and operation-line variants
    # These are nonterminals that mirror expression/operation but for single-line forms
    if @_operationNT
      for own name, type of @types when name isnt @_operationNT
        # Operation-line: has prefix rules referencing a "line" nonterminal, not left-recursive binops
        hasPrefixLine = type.rules.some (r) => r.symbols.length is 2 and not @types[r.symbols[0]] and @types[r.symbols[1]] and r.symbols[1] isnt @_exprNT
        hasNoBinOps = not type.rules.some (r) => r.symbols.length is 3 and r.symbols[0] is name
        if hasPrefixLine and hasNoBinOps and type.rules.length <= 5
          # Check if it's an alternative of a "line" expression nonterminal
          for own pName, pType of @types
            isAlt = pType.rules.some (r) => r.symbols.length is 1 and r.symbols[0] is name
            isExprAlt = pType.rules.some (r) => r.symbols.length is 1 and (r.symbols[0] is @_exprNT or r.symbols[0] is @_operationNT)
            if isAlt and isExprAlt and pName isnt @_exprNT
              @_operationLineNT ?= name
              @_exprLineNT ?= pName

    @_operationNT ?= @_exprNT
    @_valueNT ?= @_exprNT
    @_exprNT ?= @start

    # Classify Operation rules
    @_infixOps = []      # {token, bp, assoc, rule, controlTarget}
    @_prefixOps = []     # {token, bp, rule}
    @_postfixOps = []    # {token, bp, rule}
    @_ternaryOps = []    # {token, separatorToken, bp, assoc, rule}
    @_specialOps = []    # rules that don't fit simple patterns

    operationType = @types[@_operationNT]
    if operationType
      for rule in operationType.rules
        @_classifyOperationRule rule

    # Classify assignment rules — detect Expression alternatives where ALL rules
    # follow the pattern: LHS-NT TOKEN RHS-NT (with TERMINATOR/INDENT variants)
    # and share the same LHS nonterminal and operator token
    @_assignOps = []
    exprAlts = new Set
    if @_exprNT
      for rule in @types[@_exprNT].rules when rule.symbols.length is 1
        exprAlts.add rule.symbols[0]

    for altName as exprAlts
      altType = @types[altName]
      continue unless altType
      continue if altName is @_operationNT or altName is @_valueNT or altName is @_codeNT
      rules = altType.rules
      # All rules must start with a nonterminal and have a consistent second token
      continue unless rules.length >= 1
      firstLHS = rules[0].symbols[0]
      firstOP = rules[0].symbols[1]
      continue unless @types[firstLHS] and firstOP and not @types[firstOP]
      # Check consistency: most rules start with the same LHS and operator
      # (some nonterminals have a "fire-and-forget" form that starts with the op directly)
      consistentCount = rules.filter((r) => r.symbols[0] is firstLHS and r.symbols[1] is firstOP).length
      if consistentCount >= rules.length / 2
        existing = @_assignOps.find (a) -> a.token is firstOP
        unless existing
          @_assignOps.push {nonterminal: altName, token: firstOP, rule: rules[0]}

    # Find prefix starters — Expression alternatives that start with keywords
    @_prefixStarters = []
    # Tokens already handled as prefix operators
    handledTokens = new Set
    for {token: tok} in @_prefixOps
      handledTokens.add tok

    exprType = @types[@_exprNT]
    if exprType
      for rule in exprType.rules when rule.symbols.length is 1
        altName = rule.symbols[0]
        altType = @types[altName]
        continue unless altType
        # Skip Operation (handled by Pratt), Value (handled by atoms), Code (arrow functions)
        continue if altName is @_operationNT or altName is @_valueNT or altName is @_codeNT
        # Skip assignment nonterminals (handled by assignment logic)
        continue if @_assignOps.some (a) -> a.nonterminal is altName
        # Find unique keyword tokens that start this nonterminal's rules
        # (not from left-recursive rules that start with Expression/Value)
        @_findKeywordTokens altType, altName, handledTokens, new Set

    # Collect postfix chain rules — find nonterminals with left-recursive
    # property access / indexing / call rules (Value . Property, Value Arguments, etc.)
    @_postfixChains = []
    valueAlts = new Set
    if @_valueNT
      # Recursively collect all nonterminals reachable through single-NT passthroughs
      # (Value → Assignable → SimpleAssignable, Value → Invocation, etc.)
      queue = [@_valueNT]
      while queue.length
        ntName = queue.shift()
        continue if valueAlts.has ntName
        valueAlts.add ntName
        ntType = @types[ntName]
        continue unless ntType
        for rule in ntType.rules when rule.symbols.length is 1 and @types[rule.symbols[0]]
          queue.push rule.symbols[0]

    for altName as valueAlts
      altType = @types[altName]
      continue unless altType
      for rule in altType.rules
        syms = rule.symbols
        # Left-recursive through Value chain: first symbol is reachable from Value
        if syms.length >= 2 and @types[syms[0]] and (valueAlts.has(syms[0]) or syms[0] is @_codeNT)
          @_postfixChains.push {nonterminal: altName, rule}

    # Collect atom types (terminals that can start a Value)
    @_atomTokens = []
    valueType = @types[@_valueNT]
    if valueType
      @_collectAtomTokens valueType, new Set

    # Build set of tokens that parseExpression() directly handles.
    # Used by the choice generator to detect redundant alternatives.
    @_exprHandledTokens = new Set
    for {token: tok} in @_prefixStarters
      @_exprHandledTokens.add tok
    for {token: tok} in @_prefixOps
      @_exprHandledTokens.add tok
    for {token: tok} in @_atomTokens
      @_exprHandledTokens.add tok
    if @_codeNT and @firsts[@_codeNT]
      @firsts[@_codeNT].forEach (t) => @_exprHandledTokens.add t
    if @_valueNT and @firsts[@_valueNT]
      @firsts[@_valueNT].forEach (t) => @_exprHandledTokens.add t
    # Statement tokens handled by parseUnary (for 'break if done', 'return x unless err')
    @_exprHandledTokens.add 'STATEMENT'
    @_exprHandledTokens.add 'RETURN'
    @_exprHandledTokens.add 'REACT_ASSIGN'

  Generator::_findKeywordTokens = (type, dispatchName, handledTokens, visited) ->
    return if visited.has type.name
    visited.add type.name

    # Build set of nonterminals that are direct Expression/Statement alternatives
    # (these should not be followed as keyword prefixes in multi-symbol rules)
    @_exprStatementAlts ?= do =>
      alts = new Set
      for ntName in [@_exprNT, 'Statement', 'Line']
        ntType = @types[ntName]
        continue unless ntType
        alts.add ntName
        for rule in ntType.rules when rule.symbols.length is 1 and @types[rule.symbols[0]]
          alts.add rule.symbols[0]
      alts

    for rule in type.rules
      syms = rule.symbols
      first = syms[0]
      continue unless first and first isnt ''
      if @types[first]
        isExprLike = first is @_exprNT or first is @_operationNT or first is @_valueNT or first is @_codeNT
        continue if isExprLike
        # In multi-symbol rules, don't follow nonterminals that are Expression/Statement alternatives
        # (e.g., Statement in "Statement POST_IF Expression" — RETURN shouldn't be a prefix for If)
        continue if syms.length > 1 and @_exprStatementAlts.has(first)
        continue if syms.length > 1 and first is type.name  # skip self-recursive
        @_findKeywordTokens @types[first], dispatchName, handledTokens, visited
      else
        # Terminal — this is a keyword token for this nonterminal
        unless handledTokens.has first
          existing = @_prefixStarters.find (p) -> p.token is first
          unless existing
            @_prefixStarters.push {token: first, nonterminal: dispatchName}
            handledTokens.add first

  Generator::_collectAtomTokens = (type, visited) ->
    return if visited.has type.name
    visited.add type.name
    for rule in type.rules
      syms = rule.symbols
      if syms.length is 1 and not @types[syms[0]]
        # Single terminal — it's an atom
        @_atomTokens.push {token: syms[0], rule} unless @_atomTokens.some (a) -> a.token is syms[0]
      else if syms.length is 1 and @types[syms[0]]
        # Single nonterminal passthrough — recurse
        @_collectAtomTokens @types[syms[0]], visited
      else if syms.length >= 1 and not @types[syms[0]] and syms[0] isnt ''
        # Multi-symbol starting with terminal — it's an atom production
        @_atomTokens.push {token: syms[0], rule, nonterminal: type.name} unless @_atomTokens.some (a) -> a.token is syms[0]

  Generator::_classifyOperationRule = (rule) ->
    syms = rule.symbols
    return unless syms.length >= 2

    # Infix: NT TOKEN NT
    if syms.length is 3 and @types[syms[0]] and not @types[syms[1]] and @types[syms[2]]
      op = @operators[syms[1]]
      # Check for control flow: Expression || Return, Expression && Throw, etc.
      # Only flag as control-flow when the right operand is a small, keyword-led
      # nonterminal (like Return, Throw) — not when it's just a broader expression
      # nonterminal (like Expression in SimpleAssignable COMPOUND_ASSIGN Expression)
      controlTarget = null
      if syms[2] isnt syms[0]
        rightType = @types[syms[2]]
        if rightType and rightType.rules.length <= 4
          allKeywordLed = rightType.rules.every (r) => r.symbols[0] and not @types[r.symbols[0]]
          controlTarget = syms[2] if allKeywordLed
      @_infixOps.push {token: syms[1], bp: op?.precedence or 0, assoc: op?.assoc or 'left', rule, controlTarget}
      return

    # Prefix: TOKEN NT
    if syms.length is 2 and not @types[syms[0]] and @types[syms[1]]
      op = @operators[syms[0]]
      # Use rule.precedence if set (prec override, e.g., '- Expression' with prec: 'UNARY_MATH')
      bp = rule.precedence or op?.precedence or 0
      @_prefixOps.push {token: syms[0], bp, rule}
      return

    # Postfix: NT TOKEN (e.g., Value ?, SimpleAssignable ++)
    if syms.length is 2 and @types[syms[0]] and not @types[syms[1]]
      op = @operators[syms[1]]
      @_postfixOps.push {token: syms[1], bp: op?.precedence or 0, rule, leftNT: syms[0]}
      return

    # Postfix with continuation: NT TOKEN ... (e.g., SimpleAssignable COMPOUND_ASSIGN Expression)
    if syms.length >= 3 and @types[syms[0]] and not @types[syms[1]]
      op = @operators[syms[1]]
      @_postfixOps.push {token: syms[1], bp: op?.precedence or 0, rule, leftNT: syms[0]}
      return

    # Ternary: NT TOKEN NT TOKEN NT (5 symbols)
    if syms.length is 5 and @types[syms[0]] and not @types[syms[1]] and @types[syms[2]] and not @types[syms[3]] and @types[syms[4]]
      op = @operators[syms[1]]
      @_ternaryOps.push {token: syms[1], separator: syms[3], bp: op?.precedence or 0, assoc: op?.assoc or 'right', rule}
      return

    # Special/unclassified
    @_specialOps.push {rule}

  # --- Generate Pratt binding power table ---
  Generator::_generateRDBindingPowers = ->
    lines = []
    lines.push 'const BP = {};'

    for own sym, info of @operators
      # Use precedence * 2 to leave room for right-associative adjustments
      bp = info.precedence * 2
      lines.push "BP[#{JSON.stringify sym}] = #{bp};"

    lines.join '\n'

  # --- Root parser ---
  Generator::_generateRDRoot = ->
    """
    function parseRoot() {
      if (token === EOF) return withLoc(["program"]);
      const body = parseBody();
      return withLoc(["program", ...body]);
    }
    """

  # --- Body-style list (TERMINATOR-separated) ---
  Generator::_generateRDBodyList = (name) ->
    fnName = "parse#{name}"
    # Find the item nonterminal from the non-recursive single-symbol alternative
    itemNT = @_findListItem name, 'TERMINATOR'
    itemParser = "parse#{itemNT}"
    """
    function #{fnName}() {
      const items = [#{itemParser}()];
      while (token === 'TERMINATOR') {
        advance();
        if (token !== EOF && token !== 'OUTDENT') {
          items.push(#{itemParser}());
        }
      }
      return items;
    }
    """

  # --- Comma-separated list ---
  Generator::_generateRDCommaList = (name) ->
    fnName = "parse#{name}"

    # Find the item nonterminal from the non-recursive single-symbol alternative
    itemNT = @_findListItem name, ','
    itemParser = "parse#{itemNT}"

    # Check if this list supports INDENT blocks (has rules with INDENT in them)
    type = @types[name]
    hasIndent = type.rules.some (r) =>
      r.symbols.indexOf('INDENT') >= 0 or r.symbols.indexOf('OUTDENT') >= 0

    if hasIndent
      # Compute list boundary tokens from FOLLOW set (tokens that can follow this list)
      followTokens = []
      type.follows?.forEach (t) =>
        followTokens.push t unless t is ',' or t is 'TERMINATOR'
      # Always include OUTDENT as a boundary
      followTokens.push 'OUTDENT' unless followTokens.indexOf('OUTDENT') >= 0
      boundaryCond = followTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '

      """
      function #{fnName}() {
        if (token === 'INDENT') {
          advance();
          const inner = #{fnName}();
          match(',');
          expect('OUTDENT');
          return inner;
        }
        const result = [#{itemParser}()];
        while (true) {
          if (token === ',') {
            advance();
            if (#{boundaryCond}) break;
            if (token === 'TERMINATOR') {
              advance();
              if (#{boundaryCond}) break;
            }
            result.push(#{itemParser}());
          } else if (token === 'TERMINATOR') {
            advance();
            if (#{boundaryCond}) break;
            result.push(#{itemParser}());
          } else if (token === 'INDENT') {
            advance();
            const inner = #{fnName}();
            match(',');
            expect('OUTDENT');
            result.push(...inner);
          } else {
            break;
          }
        }
        return result;
      }
      """
    else
      # Simple comma-separated list (no INDENT support)
      """
      function #{fnName}() {
        const result = [#{itemParser}()];
        while (token === ',') {
          advance();
          result.push(#{itemParser}());
        }
        return result;
      }
      """

  # --- Concatenation list (no separator, just sequential items) ---
  Generator::_generateRDConcatList = (name) ->
    fnName = "parse#{name}"

    # Find the item nonterminal from the concat-recursive rule: Name Item → [Name, Item]
    type = @types[name]
    itemNT = null
    for rule in type.rules
      if rule.symbols.length is 2 and rule.symbols[0] is name and @types[rule.symbols[1]]
        itemNT = rule.symbols[1]
        break
    itemNT ?= type.rules[0]?.symbols[0] if type.rules[0]?.symbols.length is 1
    itemParser = "parse#{itemNT}"

    # Determine continuation condition from the item's FIRST set
    itemFirsts = @firsts[itemNT]
    if itemFirsts?.size
      firstTokens = []
      itemFirsts.forEach (t) => firstTokens.push t
      cond = firstTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
      """
      function #{fnName}() {
        const items = [#{itemParser}()];
        while (#{cond}) {
          items.push(#{itemParser}());
        }
        return items;
      }
      """
    else
      """
      function #{fnName}() {
        const items = [#{itemParser}()];
        while (token !== EOF && token !== 'OUTDENT') {
          items.push(#{itemParser}());
        }
        return items;
      }
      """

  # --- Choice nonterminal (dispatch on FIRST set) ---
  Generator::_generateRDChoice = (name) ->
    fnName = "parse#{name}"
    type = @types[name]
    rules = type.rules

    lines = []
    lines.push "function #{fnName}() {"

    # Collect alternatives with their FIRST sets
    # Skip multi-symbol rules that start with Expression/Statement — those are postfix
    # patterns handled by the Pratt parser (e.g., Expression POST_IF Expression in If)
    alternatives = []
    fallthrough = null

    for rule in rules
      if rule.symbols.length is 1 and rule.symbols[0] is ''
        # Epsilon rule — used as default/fallthrough
        fallthrough = rule
        continue

      sym = rule.symbols[0]
      # Skip multi-symbol rules starting with Expression, Statement, or direct Statement alternatives
      # (postfix patterns like "Expression POST_IF Expression" are handled by the Pratt parser)
      if rule.symbols.length > 1 and @types[sym]
        continue if sym is @_exprNT or sym is @_operationNT
        # Also skip rules starting with Statement or its direct alternatives (Return, Import, etc.)
        statementNT = @types['Statement']
        if statementNT
          isStatementAlt = sym is 'Statement' or statementNT.rules.some((r) -> r.symbols.length is 1 and r.symbols[0] is sym)
          continue if isStatementAlt
      if @types[sym]
        # Nonterminal passthrough (single-symbol) or multi-symbol with structural first NT
        alternatives.push {type: 'nonterminal', name: sym, rule}
      else if sym
        # Terminal-prefixed
        alternatives.push {type: 'terminal', token: sym, rule}

    # Sort nonterminal alternatives: smallest FIRST set first (most specific → default last)
    ntAlts = (alt for alt in alternatives when alt.type is 'nonterminal')
    ntAlts.sort (a, b) =>
      sizeA = @firsts[a.name]?.size or 0
      sizeB = @firsts[b.name]?.size or 0
      sizeA - sizeB  # smallest first

    # Start with terminal-prefixed (most specific)
    # But check for overlap with nonterminal alternatives (e.g., '...' overlaps with Splat)
    firstUsed = new Set
    for alt in alternatives when alt.type is 'terminal'
      # Check if this terminal also starts a nonterminal alternative
      overlappingNT = null
      for ntAlt in alternatives when ntAlt.type is 'nonterminal'
        if @firsts[ntAlt.name]?.has(alt.token)
          overlappingNT = ntAlt.name
          break
      if alt.rule.symbols.length is 1 and overlappingNT
        # Terminal overlaps with nonterminal — try nonterminal first, fall back to terminal
        lines.push "  if (token === #{JSON.stringify alt.token}) {"
        lines.push "    const _spec = speculate(() => parse#{overlappingNT}());"
        lines.push "    if (_spec !== null) return _spec;"
        action = @_getRDAction alt.rule
        lines.push "    #{action}"
        lines.push "  }"
      else if alt.rule.symbols.length is 1
        lines.push "  if (token === #{JSON.stringify alt.token}) {"
        lines.push "    const v = tokenText; advance(); return v;"
        lines.push "  }"
      else
        lines.push "  if (token === #{JSON.stringify alt.token}) {"
        lines.push "    const l = loc();"
        lines.push @_generateRDRuleBody alt.rule, '    '
        lines.push "  }"
      firstUsed.add alt.token

    # Then nonterminal alternatives, smallest FIRST set first
    # The last (broadest) becomes the default
    # Check if the broadest alternative is the expression nonterminal
    broadestIsExpr = ntAlts.length >= 2 and ntAlts[ntAlts.length - 1].name is @_exprNT

    # Collect multi-symbol rules grouped by their starting nonterminal
    # (for generating continuations after passthrough dispatch)
    continuationRules = {}
    for alt in alternatives when alt.rule.symbols.length > 1 and @types[alt.rule.symbols[0]]
      ntName = alt.rule.symbols[0]
      continuationRules[ntName] ?= []
      continuationRules[ntName].push alt.rule

    for alt, i in ntAlts
      if i is ntAlts.length - 1
        # Broadest — use as default (skip FIRST set check)
        continue
      firsts = @firsts[alt.name]
      if firsts
        uniqueTokens = []
        firsts.forEach (t) =>
          if not firstUsed.has(t)
            uniqueTokens.push t
        # If the broadest is Expression, filter out tokens that parseExpression handles directly
        # (they're redundant — parseExpression's Pratt loop handles postfix patterns like break if done)
        if broadestIsExpr
          uniqueTokens = uniqueTokens.filter (t) => not @_exprHandledTokens.has(t)
          continue unless uniqueTokens.length  # Skip entirely if all tokens are handled
        if uniqueTokens.length
          cond = uniqueTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
          # Check if this alternative also has multi-symbol continuation rules
          contRules = continuationRules[alt.name]
          if contRules?.length
            lines.push "  if (#{cond}) {"
            lines.push "    const l = loc();"
            lines.push "    let _node_ = parse#{alt.name}();"
            # Generate continuation checks (resolve nonterminals to FIRST sets)
            for contRule in contRules
              contSym = contRule.symbols[1]
              if @types[contSym]
                contTokens = []
                @firsts[contSym]?.forEach (t) => contTokens.push t
                contCond = contTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
              else
                contCond = "token === #{JSON.stringify contSym}"
              lines.push "    if (#{contCond}) {"
              lines.push "      const _vals_ = [_node_];"
              for sym, si in contRule.symbols when si > 0
                if @types[sym] then lines.push "      _vals_.push(parse#{sym}());"
                else lines.push "      _vals_.push(expect(#{JSON.stringify sym}));"
              lines.push "      const $ = _vals_, $0 = _vals_.length - 1;"
              lines.push "      const _r = ruleActions(#{contRule.id}, _vals_, [], {});"
              lines.push "      return _r != null ? withLoc(_r, l) : _node_;"
              lines.push "    }"
            lines.push "    return _node_;"
            lines.push "  }"
          else
            lines.push "  if (#{cond}) return parse#{alt.name}();"
          for t in uniqueTokens
            firstUsed.add t

    # Default case — broadest nonterminal alternative (with continuation handling)
    if fallthrough
      action = @_getRDAction fallthrough
      lines.push "  #{action}"
    else if ntAlts.length
      defaultAlt = ntAlts[ntAlts.length - 1]
      defaultContRules = continuationRules[defaultAlt.name]
      if defaultContRules?.length
        lines.push "  {"
        lines.push "    const l = loc();"
        lines.push "    let _node_ = parse#{defaultAlt.name}();"
        for contRule in defaultContRules
          contSym = contRule.symbols[1]
          if @types[contSym]
            contTokens = []
            @firsts[contSym]?.forEach (t) => contTokens.push t
            contCond = contTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
          else
            contCond = "token === #{JSON.stringify contSym}"
          lines.push "    if (#{contCond}) {"
          lines.push "      const _vals_ = [_node_];"
          for sym, si in contRule.symbols when si > 0
            if @types[sym] then lines.push "      _vals_.push(parse#{sym}());"
            else lines.push "      _vals_.push(expect(#{JSON.stringify sym}));"
          lines.push "      const $ = _vals_, $0 = _vals_.length - 1;"
          lines.push "      const _r = ruleActions(#{contRule.id}, _vals_, [], {});"
          lines.push "      return _r != null ? withLoc(_r, l) : _node_;"
          lines.push "    }"
        lines.push "    return _node_;"
        lines.push "  }"
      else
        lines.push "  return parse#{defaultAlt.name}();"
    else if alternatives.length
      last = alternatives[alternatives.length - 1]
      lines.push "  const v = tokenText; advance(); return v;" if last.type is 'terminal'
    else
      lines.push "  throw new Error('Parse error: unexpected token ' + token + ' in #{name}');"

    lines.push "}"
    lines.join '\n'

  # --- Token passthrough (single-token nonterminal) ---
  Generator::_generateRDToken = (name) ->
    fnName = "parse#{name}"
    rule = @types[name].rules[0]
    tokenTag = rule.symbols[0]
    """
    function #{fnName}() {
      return expect(#{JSON.stringify tokenTag});
    }
    """

  # --- Left-recursive loop (e.g., IfBlock → IF Expr Block | IfBlock ELSE IF Expr Block) ---
  Generator::_generateRDLeftRecLoop = (name) ->
    fnName = "parse#{name}"
    type = @types[name]
    rules = type.rules

    # Separate base rules (non-recursive) and recursive rules
    baseRules = (r for r in rules when r.symbols[0] isnt name)
    recRules  = (r for r in rules when r.symbols[0] is name)

    lines = []
    lines.push "function #{fnName}() {"
    lines.push "  const l = loc();"

    # Parse base case
    lines.push "  // Base case"
    lines.push "  const _vals_ = [];"
    if baseRules.length is 1
      for sym in baseRules[0].symbols
        if @types[sym] then lines.push "  _vals_.push(parse#{sym}());"
        else lines.push "  _vals_.push(expect(#{JSON.stringify sym}));"
      lines.push "  let _node_ = (function() { #{@_getRDAction baseRules[0], '_vals_'} }).call({});"
    else
      lines.push @_generateRDSharedPrefix baseRules, '  '

    # Generate continuation loop for recursive rules
    if recRules.length
      # Find the continuation token (first terminal after the self-reference)
      contToken = recRules[0].symbols[1]
      contFirsts = if @types[contToken]
        tokens = []
        @firsts[contToken]?.forEach (t) => tokens.push t
        tokens
      else
        [contToken]

      cond = contFirsts.map((t) -> "token === #{JSON.stringify t}").join ' || '
      lines.push "  // Recursive continuation → loop"

      # Check if continuation needs lookahead: if the first 2+ symbols are terminals,
      # we need mark/reset to avoid consuming the first token when the second doesn't match
      # (e.g., IfBlock: ELSE IF — don't consume ELSE unless IF follows)
      needsLookahead = false
      if recRules.length is 1 and recRules[0].symbols.length >= 3
        contSyms = recRules[0].symbols[1..]
        needsLookahead = contSyms.length >= 2 and not @types[contSyms[0]] and not @types[contSyms[1]]

      lines.push "  while (#{cond}) {"

      if needsLookahead
        secondToken = recRules[0].symbols[2]
        lines.push "    const _saved_ = mark();"
        lines.push "    advance(); // consume #{contToken}"
        lines.push "    if (token !== #{JSON.stringify secondToken}) { reset(_saved_); break; }"

      if recRules.length is 1
        recRule = recRules[0]
        continuation = recRule.symbols[1..]  # everything after the self-reference
        lines.push "    const _rvals_ = [_node_];"
        if needsLookahead
          # We already advanced past the first token; now consume the second via expect
          # The first token's value was consumed by advance(), push its tag as value
          lines.push "    _rvals_.push(#{JSON.stringify contToken}); // already consumed"
          for sym, si in continuation when si >= 1  # skip first (already consumed)
            if @types[sym] then lines.push "    _rvals_.push(parse#{sym}());"
            else lines.push "    _rvals_.push(expect(#{JSON.stringify sym}));"
        else
          for sym in continuation
            if @types[sym] then lines.push "    _rvals_.push(parse#{sym}());"
            else lines.push "    _rvals_.push(expect(#{JSON.stringify sym}));"
        lines.push "    const $ = _rvals_, $0 = _rvals_.length - 1;"
        lines.push "    const _r = ruleActions(#{recRule.id}, _rvals_, [], {});"
        lines.push "    _node_ = _r != null ? withLoc(_r, l) : _node_;"
      else
        # Multiple recursive rules — disambiguate by next token after continuation
        lines.push "    const _rvals_ = [_node_];"
        # Find shared continuation prefix
        contPrefixLen = 1  # skip the self-reference (index 0)
        loop
          sym = recRules[0].symbols[contPrefixLen]
          break unless sym
          break unless recRules.every (r) -> r.symbols.length > contPrefixLen and r.symbols[contPrefixLen] is sym
          contPrefixLen++
        for i in [1...contPrefixLen]
          sym = recRules[0].symbols[i]
          if @types[sym] then lines.push "    _rvals_.push(parse#{sym}());"
          else lines.push "    _rvals_.push(expect(#{JSON.stringify sym}));"
        # Disambiguate remaining
        subFirst = true
        for recRule in recRules.sort((a, b) -> b.symbols.length - a.symbols.length)
          remaining = recRule.symbols[contPrefixLen..]
          if remaining.length is 0
            lines.push "    else {" unless subFirst
            lines.push "    #{if subFirst then '' else '  '}const $ = _rvals_, $0 = _rvals_.length - 1;"
            lines.push "    #{if subFirst then '' else '  '}const _r = ruleActions(#{recRule.id}, _rvals_, [], {});"
            lines.push "    #{if subFirst then '' else '  '}_node_ = _r != null ? withLoc(_r, l) : _node_;"
            lines.push "    }" unless subFirst
          else
            nextSym = remaining[0]
            c = if @types[nextSym] then "true" else "token === #{JSON.stringify nextSym}"
            p = if subFirst then "if" else "else if"
            subFirst = false
            lines.push "    #{p} (#{c}) {"
            for sym in remaining
              if @types[sym] then lines.push "      _rvals_.push(parse#{sym}());"
              else lines.push "      _rvals_.push(expect(#{JSON.stringify sym}));"
            lines.push "      const $ = _rvals_, $0 = _rvals_.length - 1;"
            lines.push "      const _r = ruleActions(#{recRule.id}, _rvals_, [], {});"
            lines.push "      _node_ = _r != null ? withLoc(_r, l) : _node_;"
            lines.push "    }"
          subFirst = false

      lines.push "  }"

    lines.push "  return _node_;"
    lines.push "}"
    lines.join '\n'

  # --- Keyword-prefixed nonterminal ---
  Generator::_generateRDKeyword = (name) ->
    fnName = "parse#{name}"
    type = @types[name]
    rules = type.rules

    lines = []
    lines.push "function #{fnName}() {"
    lines.push "  const l = loc();"

    # Group rules by first symbol
    groups = {}
    for rule in rules
      first = rule.symbols[0]
      groups[first] ?= []
      groups[first].push rule

    firstKey = true
    for own firstToken, ruleGroup of groups
      prefix = if firstKey then "if" else "else if"
      firstKey = false
      lines.push "  #{prefix} (token === #{JSON.stringify firstToken}) {"

      if ruleGroup.length is 1
        lines.push @_generateRDRuleBody ruleGroup[0], '    '
      else
        # Multiple rules with same first token — disambiguate by lookahead
        lines.push @_generateRDSharedPrefix ruleGroup, '    '

      lines.push "  }"

    lines.push "  else {"
    lines.push "    throw new Error('Parse error: unexpected token ' + token + ' in #{name}');"
    lines.push "  }"
    lines.push "}"
    lines.join '\n'

  # --- Sequence nonterminal (rules are sequences of symbols) ---
  Generator::_generateRDSequence = (name) ->
    fnName = "parse#{name}"
    type = @types[name]
    rules = type.rules

    lines = []
    lines.push "function #{fnName}() {"
    lines.push "  const l = loc();"

    if rules.length is 1
      lines.push @_generateRDRuleBody rules[0], '  '
    else
      # Group rules by first symbol
      groups = {}
      order = []
      for rule in rules
        first = rule.symbols[0]
        key = first or '$epsilon'
        unless groups[key]
          groups[key] = []
          order.push key
        groups[key].push rule

      firstCond = true
      epsilonGroup = null
      terminalGroups = []
      nonterminalGroups = []

      for key in order
        ruleGroup = groups[key]
        if key is '$epsilon'
          epsilonGroup = ruleGroup[0]
        else if @types[key]
          nonterminalGroups.push {key, ruleGroup}
        else
          terminalGroups.push {key, ruleGroup}

      # Process terminal-first groups first (most specific)
      for {key, ruleGroup} in terminalGroups
        prefix = if firstCond then "if" else "else if"
        firstCond = false
        lines.push "  #{prefix} (token === #{JSON.stringify key}) {"
        if ruleGroup.length is 1
          lines.push @_generateRDRuleBody ruleGroup[0], '    '
        else
          lines.push @_generateRDSharedPrefix ruleGroup, '    '
        lines.push "  }"

      # Then nonterminal-first groups — sort by FIRST set size (most specific first)
      # and check if groups overlap (merge overlapping into the broadest)
      if nonterminalGroups.length
        nonterminalGroups.sort (a, b) =>
          sizeA = @firsts[a.key]?.size or 0
          sizeB = @firsts[b.key]?.size or 0
          sizeA - sizeB  # smallest first

        for {key, ruleGroup}, i in nonterminalGroups
          firsts = @firsts[key]
          if i is nonterminalGroups.length - 1
            # Broadest group — use as default, merge all remaining rules
            allNTRules = []
            for {ruleGroup: rg} in nonterminalGroups[i..]
              allNTRules.push ...rg
            if firstCond
              lines.push @_generateRDSharedPrefix allNTRules, '  '
              firstCond = false
            else
              lines.push "  else {"
              lines.push @_generateRDSharedPrefix allNTRules, '    '
              lines.push "  }"
            break
          else if firsts?.size
            # Specific group — dispatch on unique FIRST tokens
            uniqueTokens = []
            firsts.forEach (t) =>
              # Only emit if this token isn't in broader groups
              unique = true
              for {key: otherKey} in nonterminalGroups[i + 1..]
                otherFirsts = @firsts[otherKey]
                if otherFirsts?.has(t)
                  unique = false
                  break
              uniqueTokens.push t if unique
            if uniqueTokens.length
              cond = uniqueTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
              prefix = if firstCond then "if" else "else if"
              firstCond = false
              lines.push "  #{prefix} (#{cond}) {"
              lines.push @_generateRDSharedPrefix ruleGroup, '    '
              lines.push "  }"

      if epsilonGroup
        if firstCond
          lines.push @_generateRDRuleBody epsilonGroup, '  '
        else
          lines.push "  else {"
          lines.push @_generateRDRuleBody epsilonGroup, '    '
          lines.push "  }"

    lines.push "}"
    lines.join '\n'

  # --- Generate code for rules sharing a common prefix ---
  Generator::_generateRDSharedPrefix = (rules, indent) ->
    return @_generateRDRuleBody rules[0], indent if rules.length is 1

    # Find longest common prefix
    prefixLen = 0
    loop
      sym = rules[0].symbols[prefixLen]
      break unless sym and sym isnt ''
      break unless rules.every (r) -> r.symbols.length > prefixLen and r.symbols[prefixLen] is sym
      prefixLen++

    lines = []

    if prefixLen is 0
      # No common prefix — just use first rule as default
      lines.push @_generateRDRuleBody rules[0], indent
      return lines.join '\n'

    # Parse common prefix into vals
    lines.push "#{indent}const _vals_ = [];"
    for i in [0...prefixLen]
      sym = rules[0].symbols[i]
      if @types[sym]
        lines.push "#{indent}_vals_.push(parse#{sym}());"
      else
        lines.push "#{indent}_vals_.push(expect(#{JSON.stringify sym}));"

    # Categorize suffixes (what comes after the common prefix)
    sorted = rules[..].sort (a, b) -> b.symbols.length - a.symbols.length
    terminalSuffixes = []
    nonterminalSuffixes = []
    emptyRule = null

    for rule in sorted
      remaining = rule.symbols[prefixLen..]
      if remaining.length is 0
        emptyRule = rule
      else if @types[remaining[0]]
        nonterminalSuffixes.push rule
      else
        terminalSuffixes.push rule

    # Check if suffixes form an "optional chain" pattern:
    # Multiple rules that are supersets of shorter rules with an empty base case.
    # Uses rule-length-based dispatch to select the correct semantic action.
    # Only valid when ALL suffix symbols appear in the longest rule's suffix
    # (otherwise nonterminal suffixes like [Expression] would be missed).
    isOptionalChain = false
    if emptyRule and (nonterminalSuffixes.length + terminalSuffixes.length) >= 2
      longestSuffix = sorted[0].symbols[prefixLen..]
      longestSyms = new Set longestSuffix
      isOptionalChain = sorted.every (rule) =>
        suffix = rule.symbols[prefixLen..]
        suffix.every (sym) => longestSyms.has sym

    if isOptionalChain
      # Optional chain: parse each optional part if its FIRST token matches
      # Then select the correct rule based on how many symbols were parsed
      longestRule = sorted[0]
      longestRemaining = longestRule.symbols[prefixLen..]

      # Generate optional checks using FIRST sets
      for sym in longestRemaining
        if @types[sym]
          firsts = @firsts[sym]
          if firsts?.size
            firstTokens = []
            firsts.forEach (t) => firstTokens.push t
            cond = firstTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
            lines.push "#{indent}if (#{cond}) _vals_.push(parse#{sym}()); // optional"
          else
            lines.push "#{indent}_vals_.push(parse#{sym}());"
        else
          lines.push "#{indent}if (token === #{JSON.stringify sym}) _vals_.push(expect(#{JSON.stringify sym})); // optional"

      # Select the correct rule based on how many symbols were actually parsed
      # Build a lookup from vals length to rule ID
      ruleEntries = []
      for rule in sorted
        ruleEntries.push "#{rule.symbols.length}:#{rule.id}"
      mapStr = "{" + ruleEntries.join(",") + "}"
      lines.push "#{indent}const _rid_ = (#{mapStr})[_vals_.length];"
      lines.push "#{indent}if (_rid_ !== undefined) {"
      lines.push "#{indent}  const $ = _vals_, $0 = _vals_.length - 1;"
      lines.push "#{indent}  const _r = ruleActions(_rid_, _vals_, [], {});"
      lines.push "#{indent}  return _r != null ? withLoc(_r, l) : withLoc($[$0], l);"
      lines.push "#{indent}}"
      lines.push "#{indent}#{@_getRDAction longestRule, '_vals_'}"
    else
      # Standard disambiguation: terminal suffixes first, then nonterminal default
      firstCond = true

      # Group terminal suffixes by their first token (to avoid duplicate branches)
      termGroups = {}
      termOrder = []
      for rule in terminalSuffixes
        remaining = rule.symbols[prefixLen..]
        tok = remaining[0]
        unless termGroups[tok]
          termGroups[tok] = []
          termOrder.push tok
        termGroups[tok].push rule

      for tok in termOrder
        group = termGroups[tok]
        prefix = if firstCond then "if" else "else if"
        firstCond = false
        lines.push "#{indent}#{prefix} (token === #{JSON.stringify tok}) {"
        if group.length is 1
          remaining = group[0].symbols[prefixLen..]
          for sym in remaining
            if @types[sym]
              lines.push "#{indent}  _vals_.push(parse#{sym}());"
            else
              lines.push "#{indent}  _vals_.push(expect(#{JSON.stringify sym}));"
          lines.push "#{indent}  #{@_getRDAction group[0], '_vals_'}"
        else
          # Multiple rules with same first token — find deeper shared prefix
          # and disambiguate at the divergence point
          subPrefixLen = prefixLen
          loop
            sym = group[0].symbols[subPrefixLen]
            break unless sym
            break unless group.every (r) -> r.symbols.length > subPrefixLen and r.symbols[subPrefixLen] is sym
            subPrefixLen++
          # Parse the shared sub-prefix
          for i in [prefixLen...subPrefixLen]
            sym = group[0].symbols[i]
            if @types[sym]
              lines.push "#{indent}  _vals_.push(parse#{sym}());"
            else
              lines.push "#{indent}  _vals_.push(expect(#{JSON.stringify sym}));"
          # Disambiguate at the divergence point
          subFirst = true
          for rule in group.sort((a, b) -> b.symbols.length - a.symbols.length)
            remaining = rule.symbols[subPrefixLen..]
            if remaining.length is 0
              if subFirst
                lines.push "#{indent}  #{@_getRDAction rule, '_vals_'}"
              else
                lines.push "#{indent}  else {"
                lines.push "#{indent}    #{@_getRDAction rule, '_vals_'}"
                lines.push "#{indent}  }"
            else
              nextSym = remaining[0]
              cond = if @types[nextSym]
                firsts = @firsts[nextSym]
                if firsts?.size
                  tokens = []
                  firsts.forEach (t) => tokens.push t
                  tokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
                else
                  "true"
              else
                "token === #{JSON.stringify nextSym}"
              p = if subFirst then "if" else "else if"
              subFirst = false
              lines.push "#{indent}  #{p} (#{cond}) {"
              for sym in remaining
                if @types[sym]
                  lines.push "#{indent}    _vals_.push(parse#{sym}());"
                else
                  lines.push "#{indent}    _vals_.push(expect(#{JSON.stringify sym}));"
              lines.push "#{indent}    #{@_getRDAction rule, '_vals_'}"
              lines.push "#{indent}  }"
            subFirst = false
        lines.push "#{indent}}"

      # Nonterminal suffixes as default (or with FIRST check when emptyRule exists)
      if nonterminalSuffixes.length
        inner = if firstCond then indent else "#{indent}  "
        lines.push "#{indent}else {" unless firstCond

        if nonterminalSuffixes.length is 1
          remaining = nonterminalSuffixes[0].symbols[prefixLen..]
          # When emptyRule exists, wrap nonterminal in FIRST check so empty is the default
          if emptyRule and remaining.length and remaining[0] and @types[remaining[0]]
            ntFirsts = @firsts[remaining[0]]
            if ntFirsts?.size
              ntTokens = []
              ntFirsts.forEach (t) => ntTokens.push t
              ntCond = ntTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
              lines.push "#{inner}if (#{ntCond}) {"
              for sym in remaining
                if @types[sym] then lines.push "#{inner}  _vals_.push(parse#{sym}());"
                else lines.push "#{inner}  _vals_.push(expect(#{JSON.stringify sym}));"
              lines.push "#{inner}  #{@_getRDAction nonterminalSuffixes[0], '_vals_'}"
              lines.push "#{inner}}"
              # emptyRule as else of the FIRST check
              lines.push "#{inner}else {"
              lines.push "#{inner}  #{@_getRDAction emptyRule, '_vals_'}"
              lines.push "#{inner}}"
              emptyRule = null  # Consumed — don't emit again below
            else
              for sym in remaining
                if @types[sym] then lines.push "#{inner}_vals_.push(parse#{sym}());"
                else lines.push "#{inner}_vals_.push(expect(#{JSON.stringify sym}));"
              lines.push "#{inner}#{@_getRDAction nonterminalSuffixes[0], '_vals_'}"
          else
            for sym in remaining
              if @types[sym] then lines.push "#{inner}_vals_.push(parse#{sym}());"
              else lines.push "#{inner}_vals_.push(expect(#{JSON.stringify sym}));"
            lines.push "#{inner}#{@_getRDAction nonterminalSuffixes[0], '_vals_'}"
        else
          # Multiple nonterminal suffixes — find deeper shared prefix (extending
          # through nonterminals) until we reach a terminal divergence point
          subPrefixLen = prefixLen
          loop
            sym = nonterminalSuffixes[0].symbols[subPrefixLen]
            break unless sym
            break unless nonterminalSuffixes.every (r) -> r.symbols.length > subPrefixLen and r.symbols[subPrefixLen] is sym
            subPrefixLen++
          for i in [prefixLen...subPrefixLen]
            sym = nonterminalSuffixes[0].symbols[i]
            if @types[sym] then lines.push "#{inner}_vals_.push(parse#{sym}());"
            else lines.push "#{inner}_vals_.push(expect(#{JSON.stringify sym}));"
          # Re-categorize remaining suffixes at the new divergence point
          subTerminals = []
          subNonterminals = []
          subEmpty = null
          for rule in nonterminalSuffixes.sort((a, b) -> b.symbols.length - a.symbols.length)
            remaining = rule.symbols[subPrefixLen..]
            if remaining.length is 0 then subEmpty = rule
            else if @types[remaining[0]] then subNonterminals.push rule
            else subTerminals.push rule
          # Generate terminal branches first
          subFirst = true
          subTermGroups = {}
          subTermOrder = []
          for rule in subTerminals
            tok = rule.symbols[subPrefixLen]
            unless subTermGroups[tok]
              subTermGroups[tok] = []
              subTermOrder.push tok
            subTermGroups[tok].push rule
          for tok in subTermOrder
            group = subTermGroups[tok]
            p = if subFirst then "if" else "else if"
            subFirst = false
            lines.push "#{inner}#{p} (token === #{JSON.stringify tok}) {"
            if group.length is 1
              for sym in group[0].symbols[subPrefixLen..]
                if @types[sym] then lines.push "#{inner}  _vals_.push(parse#{sym}());"
                else lines.push "#{inner}  _vals_.push(expect(#{JSON.stringify sym}));"
              lines.push "#{inner}  #{@_getRDAction group[0], '_vals_'}"
            else
              # Deeper shared prefix within this group
              deepLen = subPrefixLen
              loop
                sym = group[0].symbols[deepLen]
                break unless sym
                break unless group.every (r) -> r.symbols.length > deepLen and r.symbols[deepLen] is sym
                deepLen++
              for i in [subPrefixLen...deepLen]
                sym = group[0].symbols[i]
                if @types[sym] then lines.push "#{inner}  _vals_.push(parse#{sym}());"
                else lines.push "#{inner}  _vals_.push(expect(#{JSON.stringify sym}));"
              # Check for optional chain within group (empty remaining = shortest)
              hasShort = group.some (r) -> r.symbols.length is deepLen
              if hasShort
                longest = group.sort((a, b) -> b.symbols.length - a.symbols.length)[0]
                for sym2 in longest.symbols[deepLen..]
                  if @types[sym2]
                    firsts2 = @firsts[sym2]
                    if firsts2?.size
                      toks2 = []
                      firsts2.forEach (t2) => toks2.push t2
                      cond2 = toks2.map((t2) -> "token === #{JSON.stringify t2}").join ' || '
                      lines.push "#{inner}  if (#{cond2}) _vals_.push(parse#{sym2}());"
                    else
                      lines.push "#{inner}  _vals_.push(parse#{sym2}());"
                  else
                    lines.push "#{inner}  if (token === #{JSON.stringify sym2}) _vals_.push(expect(#{JSON.stringify sym2}));"
                lines.push "#{inner}  #{@_getRDAction longest, '_vals_'}"
              else
                lines.push "#{inner}  #{@_getRDAction group[0], '_vals_'}"
            lines.push "#{inner}}"
          # Nonterminal suffixes — group by first symbol and dispatch on FIRST sets
          if subNonterminals.length
            # Group by first remaining symbol
            ntGroups = {}
            ntOrder = []
            for rule in subNonterminals
              key = rule.symbols[subPrefixLen]
              unless ntGroups[key]
                ntGroups[key] = []
                ntOrder.push key
              ntGroups[key].push rule
            # Sort by FIRST set size (smallest first, largest = default)
            ntOrder.sort (a, b) => (@firsts[a]?.size or 0) - (@firsts[b]?.size or 0)
            for ntKey, ni in ntOrder
              group = ntGroups[ntKey]
              if ni is ntOrder.length - 1 and ntOrder.length is 1
                # Only one group — use as default
                base = if subFirst and not subEmpty then inner else "#{inner}  "
                lines.push "#{inner}else {" unless subFirst and not subEmpty
                if group.length is 1
                  for sym in group[0].symbols[subPrefixLen..]
                    if @types[sym] then lines.push "#{base}_vals_.push(parse#{sym}());"
                    else lines.push "#{base}_vals_.push(expect(#{JSON.stringify sym}));"
                  lines.push "#{base}#{@_getRDAction group[0], '_vals_'}"
                else
                  # Multiple rules — find deeper shared prefix within group, then disambiguate
                  deepLen = subPrefixLen
                  loop
                    sym = group[0].symbols[deepLen]
                    break unless sym
                    break unless group.every (r) -> r.symbols.length > deepLen and r.symbols[deepLen] is sym
                    deepLen++
                  for i in [subPrefixLen...deepLen]
                    sym = group[0].symbols[i]
                    if @types[sym] then lines.push "#{base}_vals_.push(parse#{sym}());"
                    else lines.push "#{base}_vals_.push(expect(#{JSON.stringify sym}));"
                  # Disambiguate remaining by terminal token
                  deepFirst = true
                  deepSorted = group[..].sort (a, b) -> b.symbols.length - a.symbols.length
                  deepEmpty = null
                  for rule in deepSorted
                    remaining = rule.symbols[deepLen..]
                    if remaining.length is 0
                      deepEmpty = rule
                    else
                      tok = remaining[0]
                      cond = if @types[tok]
                        toks = []
                        @firsts[tok]?.forEach (t) => toks.push t
                        toks.map((t) -> "token === #{JSON.stringify t}").join(' || ') or "true"
                      else "token === #{JSON.stringify tok}"
                      p = if deepFirst then "if" else "else if"
                      deepFirst = false
                      lines.push "#{base}#{p} (#{cond}) {"
                      for sym in remaining
                        if @types[sym] then lines.push "#{base}  _vals_.push(parse#{sym}());"
                        else lines.push "#{base}  _vals_.push(expect(#{JSON.stringify sym}));"
                      lines.push "#{base}  #{@_getRDAction rule, '_vals_'}"
                      lines.push "#{base}}"
                  if deepEmpty
                    if deepFirst
                      lines.push "#{base}#{@_getRDAction deepEmpty, '_vals_'}"
                    else
                      lines.push "#{base}else { #{@_getRDAction deepEmpty, '_vals_'} }"
                  else unless deepFirst
                    lines.push "#{base}#{@_getRDAction deepSorted[0], '_vals_'}"
                lines.push "#{inner}}" unless subFirst and not subEmpty
              else
                # Specific group — dispatch on unique FIRST tokens
                firsts = @firsts[ntKey]
                if firsts?.size
                  uniqueTokens = []
                  firsts.forEach (t) => uniqueTokens.push t
                  cond = uniqueTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
                  p = if subFirst then "if" else "else if"
                  subFirst = false
                  lines.push "#{inner}#{p} (#{cond}) {"
                  if group.length is 1
                    for sym in group[0].symbols[subPrefixLen..]
                      if @types[sym] then lines.push "#{inner}  _vals_.push(parse#{sym}());"
                      else lines.push "#{inner}  _vals_.push(expect(#{JSON.stringify sym}));"
                    lines.push "#{inner}  #{@_getRDAction group[0], '_vals_'}"
                  else
                    # Multiple rules — find deeper shared prefix, then disambiguate
                    deepLen = subPrefixLen
                    loop
                      sym = group[0].symbols[deepLen]
                      break unless sym
                      break unless group.every (r) -> r.symbols.length > deepLen and r.symbols[deepLen] is sym
                      deepLen++
                    for i in [subPrefixLen...deepLen]
                      sym = group[0].symbols[i]
                      if @types[sym] then lines.push "#{inner}  _vals_.push(parse#{sym}());"
                      else lines.push "#{inner}  _vals_.push(expect(#{JSON.stringify sym}));"
                    deepFirst = true
                    for rule in group.sort((a, b) -> b.symbols.length - a.symbols.length)
                      remaining = rule.symbols[deepLen..]
                      if remaining.length is 0
                        unless deepFirst
                          lines.push "#{inner}  else {"
                        lines.push "#{inner}  #{if deepFirst then '' else '  '}#{@_getRDAction rule, '_vals_'}"
                        lines.push "#{inner}  }" unless deepFirst
                      else
                        tok2 = remaining[0]
                        cond2 = if @types[tok2]
                          t2 = []; @firsts[tok2]?.forEach (t) => t2.push t
                          t2.map((t) -> "token === #{JSON.stringify t}").join(' || ') or "true"
                        else "token === #{JSON.stringify tok2}"
                        dp = if deepFirst then "if" else "else if"
                        deepFirst = false
                        lines.push "#{inner}  #{dp} (#{cond2}) {"
                        for sym in remaining
                          if @types[sym] then lines.push "#{inner}    _vals_.push(parse#{sym}());"
                          else lines.push "#{inner}    _vals_.push(expect(#{JSON.stringify sym}));"
                        lines.push "#{inner}    #{@_getRDAction rule, '_vals_'}"
                        lines.push "#{inner}  }"
                      deepFirst = false
                  lines.push "#{inner}}"
          else if subEmpty
            if subFirst
              lines.push "#{inner}#{@_getRDAction subEmpty, '_vals_'}"
            else
              lines.push "#{inner}else {"
              lines.push "#{inner}  #{@_getRDAction subEmpty, '_vals_'}"
              lines.push "#{inner}}"

        lines.push "#{indent}}" unless firstCond
      else if emptyRule
        if firstCond
          lines.push "#{indent}#{@_getRDAction emptyRule, '_vals_'}"
        else
          lines.push "#{indent}else {"
          lines.push "#{indent}  #{@_getRDAction emptyRule, '_vals_'}"
          lines.push "#{indent}}"

    lines.join '\n'

  # --- Generate body for a single grammar rule ---
  Generator::_generateRDRuleBody = (rule, indent = '  ') ->
    {symbols} = rule
    lines = []
    valsVar = '_vals_'

    # Handle epsilon
    if symbols.length is 0 or (symbols.length is 1 and symbols[0] is '')
      action = @_getRDAction rule
      lines.push "#{indent}#{action}"
      return lines.join '\n'

    lines.push "#{indent}const #{valsVar} = [];"

    for sym in symbols
      if @types[sym]
        lines.push "#{indent}#{valsVar}.push(parse#{sym}());"
      else
        lines.push "#{indent}#{valsVar}.push(expect(#{JSON.stringify sym}));"

    # Apply semantic action
    action = @_getRDAction rule, valsVar
    lines.push "#{indent}#{action}"

    lines.join '\n'

  # --- Get the semantic action code for a rule ---
  Generator::_getRDAction = (rule, valsVar = '_vals_') ->
    {symbols} = rule
    len = if symbols[0] is '' then 0 else symbols.length

    if len is 0
      "{ const $ = [], $0 = -1; const _r = ruleActions(#{rule.id}, [], [], {}); return _r != null ? withLoc(_r, l) : withLoc([], l); }"
    else
      "{ const $ = #{valsVar}, $0 = #{valsVar}.length - 1; const _r = ruleActions(#{rule.id}, #{valsVar}, [], {}); return _r != null ? withLoc(_r, l) : withLoc($[$0], l); }"

  # --- Expression parser (Pratt-based, GENERATED from grammar rules) ---
  Generator::_generateRDExpression = ->
    lines = []
    lines.push "function parseExpression(minBP) {"
    lines.push "  if (minBP === undefined) minBP = 0;"
    lines.push "  const l = loc();"
    lines.push ""

    # Prefix starters — keyword-led expressions (IF, FOR, CLASS, etc.)
    lines.push "  // Statement-like expression starters (derived from Expression alternatives)"
    for {token, nonterminal} in @_prefixStarters
      lines.push "  if (token === #{JSON.stringify token}) return parse#{nonterminal}();"
    lines.push ""

    # Arrow function detection
    codeFirsts = if @_codeNT then @firsts[@_codeNT] else null
    if codeFirsts
      codeTokens = []
      codeFirsts.forEach (t) => codeTokens.push t
      if codeTokens.length
        cond = codeTokens.map((t) -> "token === #{JSON.stringify t}").join ' || '
        lines.push "  // Arrow functions"
        lines.push "  if (#{cond}) return parse#{@_codeNT}();"
        lines.push ""

    lines.push "  // Pratt expression parser"
    lines.push "  let left = parseUnary();"
    lines.push ""
    lines.push "  while (true) {"

    # Assignment operators (lowest precedence, right-associative)
    if @_assignOps.length
      lines.push "    // Assignment operators (derived from grammar rules)"
      lines.push "    if (minBP === 0) {"
      for {token: tok, rule} in @_assignOps
        lines.push "      if (token === #{JSON.stringify tok}) {"
        lines.push "        const _op = tokenText; advance();"
        lines.push "        let right;"
        lines.push "        if (token === 'TERMINATOR') { advance(); right = parseExpression(0); }"
        lines.push "        else if (token === 'INDENT') { advance(); right = parseExpression(0); expect('OUTDENT'); }"
        lines.push "        else { right = parseExpression(0); }"
        # Use ruleActions with constructed vals
        lines.push "        const _vals_ = [left, _op, right];"
        lines.push "        const $ = _vals_, $0 = _vals_.length - 1;"
        lines.push "        const _r = ruleActions(#{rule.id}, _vals_, [], {});"
        lines.push "        left = _r != null ? withLoc(_r, l) : withLoc(left, l);"
        lines.push "        continue;"
        lines.push "      }"
      lines.push "    }"
      lines.push ""

    # Postfix operators (from Operation rules)
    # Skip tokens that also appear as infix ops (they're handled there with INDENT/TERMINATOR variants)
    infixTokens = new Set
    for {token: t} in @_infixOps
      infixTokens.add t
    if @_postfixOps.length
      lines.push "    // Postfix operators (derived from Operation rules)"
      seen = new Set
      for {token: tok, bp, rule, leftNT} in @_postfixOps
        continue if seen.has tok
        continue if infixTokens.has tok
        seen.add tok
        lines.push "    if (token === #{JSON.stringify tok} && (BP[#{JSON.stringify tok}] || 0) >= minBP) {"
        # Build vals for the action
        syms = rule.symbols
        lines.push "      const _vals_ = [left];"
        for sym, i in syms when i > 0
          if @types[sym]
            lines.push "      _vals_.push(parse#{sym}());"
          else
            lines.push "      _vals_.push(expect(#{JSON.stringify sym}));"
        lines.push "      const $ = _vals_, $0 = _vals_.length - 1;"
        lines.push "      const _r = ruleActions(#{rule.id}, _vals_, [], {});"
        lines.push "      left = _r != null ? withLoc(_r, l) : left;"
        lines.push "      continue;"
        lines.push "    }"

    # Postfix chains (property access, indexing, calls — from SimpleAssignable/Invocation)
    if @_postfixChains.length
      lines.push ""
      lines.push "    // Postfix chains (derived from SimpleAssignable/Invocation rules)"

      # Group rules by effective dispatch token (resolving nonterminals to FIRST sets)
      chainGroups = {}
      chainOrder = []
      for {nonterminal, rule} in @_postfixChains
        syms = rule.symbols
        chainSym = syms[1]
        # Resolve nonterminal to its FIRST set tokens
        tokens = []
        if @types[chainSym]
          @firsts[chainSym]?.forEach (t) => tokens.push t
        else
          tokens.push chainSym
        for tok in tokens
          unless chainGroups[tok]
            chainGroups[tok] = []
            chainOrder.push tok
          # Avoid duplicate rules in same group
          unless chainGroups[tok].some (entry) -> entry.rule.id is rule.id
            chainGroups[tok].push {nonterminal, rule}

      emittedChains = new Set
      for tok in chainOrder
        group = chainGroups[tok]
        continue if emittedChains.has tok
        emittedChains.add tok
        # Use operator binding power if available, otherwise always match
        bp = @operators[tok]
        bpCheck = if bp then " && (BP[#{JSON.stringify tok}] || 0) >= minBP" else ""
        lines.push "    if (token === #{JSON.stringify tok}#{bpCheck}) {"
        if group.length is 1
          {rule} = group[0]
          syms = rule.symbols
          lines.push "      const _vals_ = [left];"
          for sym, i in syms when i > 0
            if @types[sym]
              lines.push "      _vals_.push(parse#{sym}());"
            else
              lines.push "      _vals_.push(expect(#{JSON.stringify sym}));"
          lines.push "      const $ = _vals_, $0 = _vals_.length - 1;"
          lines.push "      const _r = ruleActions(#{rule.id}, _vals_, [], {});"
          lines.push "      left = _r != null ? withLoc(_r, l) : left;"
        else
          # Check if this group has both Expression and Slice rules (INDEX_START ambiguity)
          # If so, use speculation: try each rule with Slice first, fall back to Expression
          hasSlice = group.some (g) => g.rule.symbols.some (s) => s is 'Slice'
          hasExpr = group.some (g) => g.rule.symbols.some (s) => s is @_exprNT
          if hasSlice and hasExpr
            # Index with Slice/Expression ambiguity: parse Expression, then check for RangeDots
            # If RangeDots follows, it's a Slice; otherwise it's a plain index
            exprRules = (g.rule for g in group when not g.rule.symbols.some (s) => s is 'Slice')
            sliceRules = (g.rule for g in group when g.rule.symbols.some (s) => s is 'Slice')
            sliceRule = sliceRules.sort((a, b) -> a.symbols.length - b.symbols.length)[0]
            exprRule = exprRules.sort((a, b) -> a.symbols.length - b.symbols.length)[0]
            lines.push "      const _vals_ = [left];"
            lines.push "      _vals_.push(expect(#{JSON.stringify tok}));"
            # Check for RangeDots first (Slice starting with ..)
            lines.push "      if (token === '..' || token === '...') {"
            lines.push "        const _slice = parseRangeDots();"
            lines.push "        if (token !== 'INDEX_END') { _vals_.push([_slice, null, parseExpression()]); }"
            lines.push "        else { _vals_.push([_slice, null, null]); }"
            lines.push "        _vals_.push(expect('INDEX_END'));"
            lines.push "        const $ = _vals_, $0 = _vals_.length - 1;"
            lines.push "        const _r = ruleActions(#{sliceRule.id}, _vals_, [], {});"
            lines.push "        left = _r != null ? withLoc(_r, l) : left; continue;"
            lines.push "      }"
            # Parse Expression, then check for RangeDots (Slice: expr..expr, expr..)
            lines.push "      const _expr = parseExpression();"
            lines.push "      if (token === '..' || token === '...') {"
            lines.push "        const _dots = parseRangeDots();"
            lines.push "        if (token !== 'INDEX_END') {"
            lines.push "          _vals_.push([_dots, _expr, parseExpression()]);"
            lines.push "        } else {"
            lines.push "          _vals_.push([_dots, _expr, null]);"
            lines.push "        }"
            lines.push "        _vals_.push(expect('INDEX_END'));"
            lines.push "        const $ = _vals_, $0 = _vals_.length - 1;"
            lines.push "        const _r = ruleActions(#{sliceRule.id}, _vals_, [], {});"
            lines.push "        left = _r != null ? withLoc(_r, l) : left;"
            lines.push "      } else {"
            lines.push "        _vals_.push(_expr);"
            lines.push "        _vals_.push(expect('INDEX_END'));"
            lines.push "        const $ = _vals_, $0 = _vals_.length - 1;"
            lines.push "        const _r = ruleActions(#{exprRule.id}, _vals_, [], {});"
            lines.push "        left = _r != null ? withLoc(_r, l) : left;"
            lines.push "      }"
            lines.push "      continue;"
            lines.push "    }"
            continue  # Skip normal multi-rule handling for this group

          # Multiple rules — find common prefix after syms[0] (the left NT), then disambiguate
          lines.push "      const _vals_ = [left];"
          # Find shared prefix (starting from index 1)
          prefixLen = 1
          loop
            sym = group[0].rule.symbols[prefixLen]
            break unless sym
            break unless group.every (g) -> g.rule.symbols.length > prefixLen and g.rule.symbols[prefixLen] is sym
            prefixLen++
          for i in [1...prefixLen]
            sym = group[0].rule.symbols[i]
            if @types[sym] then lines.push "      _vals_.push(parse#{sym}());"
            else lines.push "      _vals_.push(expect(#{JSON.stringify sym}));"
          # Disambiguate remaining suffixes (deduplicate rules with identical suffixes)
          sorted = group[..].sort (a, b) -> b.rule.symbols.length - a.rule.symbols.length
          # Deduplicate: only keep one rule per unique suffix
          uniqueSorted = []
          seenSuffixes = new Set
          for entry in sorted
            suffix = entry.rule.symbols[prefixLen..].join(' ')
            unless seenSuffixes.has suffix
              seenSuffixes.add suffix
              uniqueSorted.push entry
          subFirst = true
          for {rule: r} in uniqueSorted
            remaining = r.symbols[prefixLen..]
            if remaining.length is 0
              unless subFirst
                lines.push "      else {"
              lines.push "#{if subFirst then '      ' else '        '}const $ = _vals_, $0 = _vals_.length - 1;"
              lines.push "#{if subFirst then '      ' else '        '}const _r = ruleActions(#{r.id}, _vals_, [], {});"
              lines.push "#{if subFirst then '      ' else '        '}left = _r != null ? withLoc(_r, l) : left;"
              unless subFirst
                lines.push "      }"
            else
              nextSym = remaining[0]
              cond = if @types[nextSym]
                firsts = @firsts[nextSym]
                if firsts?.size
                  toks = []
                  firsts.forEach (t) => toks.push t
                  toks.map((t) -> "token === #{JSON.stringify t}").join ' || '
                else "true"
              else "token === #{JSON.stringify nextSym}"
              p = if subFirst then "if" else "else if"
              subFirst = false
              lines.push "      #{p} (#{cond}) {"
              for sym in remaining
                if @types[sym] then lines.push "        _vals_.push(parse#{sym}());"
                else lines.push "        _vals_.push(expect(#{JSON.stringify sym}));"
              lines.push "        const $ = _vals_, $0 = _vals_.length - 1;"
              lines.push "        const _r = ruleActions(#{r.id}, _vals_, [], {});"
              lines.push "        left = _r != null ? withLoc(_r, l) : left;"
              lines.push "      }"
            subFirst = false
        lines.push "      continue;"
        lines.push "    }"

    # Infix binary operators (from Operation rules)
    lines.push ""
    lines.push "    // Binary operators (derived from Operation rules)"
    seen = new Set
    for {token: tok, bp, assoc, rule, controlTarget} in @_infixOps
      continue if seen.has tok
      bpExpr = "(BP[#{JSON.stringify tok}] || 0)"
      nextBP = if assoc is 'right' then bpExpr else "#{bpExpr} + 1"

      # Check if this operator has control-flow variants (|| Return, && Throw, etc.)
      controlOps = @_infixOps.filter (op) -> op.token is tok and op.controlTarget
      normalRule = @_infixOps.find (op) -> op.token is tok and not op.controlTarget

      if controlOps.length
        # Emit a merged handler: check each control target, fall through to normal binary
        seen.add tok
        lines.push "    if (token === #{JSON.stringify tok} && #{bpExpr} >= minBP) {"
        lines.push "      advance();"
        for ctrlOp, ci in controlOps
          prefix = if ci is 0 then "if" else "else if"
          lines.push "      #{prefix} (token === #{JSON.stringify ctrlOp.controlTarget.toUpperCase()}) {"
          lines.push "        const ctrl = parse#{ctrlOp.controlTarget}();"
          lines.push "        const _vals_ = [left, #{JSON.stringify tok}, ctrl];"
          lines.push "        const $ = _vals_, $0 = 2;"
          lines.push "        const _r = ruleActions(#{ctrlOp.rule.id}, _vals_, [], {});"
          lines.push "        left = _r != null ? withLoc(_r, l) : left; continue;"
          lines.push "      }"
        if normalRule
          lines.push "      else {"
          lines.push "        const right = parseExpression(#{nextBP});"
          lines.push "        const _vals_ = [left, #{JSON.stringify tok}, right];"
          lines.push "        const $ = _vals_, $0 = 2;"
          lines.push "        const _r = ruleActions(#{normalRule.rule.id}, _vals_, [], {});"
          lines.push "        left = _r != null ? withLoc(_r, l) : left; continue;"
          lines.push "      }"
        lines.push "    }"
        continue

      # No control-flow variants — normal binary operator
      seen.add tok
      lines.push "    if (token === #{JSON.stringify tok} && #{bpExpr} >= minBP) {"
      # Check if token is a category token (MATH, COMPARE, etc.) where tokenText is the actual op
      isCategoryToken = tok is tok.toUpperCase() and tok.length > 2
      if isCategoryToken
        lines.push "      const _op = tokenText; advance();"
      else
        lines.push "      advance();"
      # Check if this operator has TERMINATOR/INDENT variants in the grammar
      hasIndentVariant = @_postfixOps.some (op) -> op.token is tok
      if hasIndentVariant
        lines.push "      let right;"
        lines.push "      if (token === 'TERMINATOR') { advance(); right = parseExpression(#{nextBP}); }"
        lines.push "      else if (token === 'INDENT') { advance(); right = parseExpression(#{nextBP}); expect('OUTDENT'); }"
        lines.push "      else { right = parseExpression(#{nextBP}); }"
      else
        lines.push "      const right = parseExpression(#{nextBP});"
      lines.push "      const _vals_ = [left, #{if isCategoryToken then '_op' else JSON.stringify tok}, right];"
      lines.push "      const $ = _vals_, $0 = 2;"
      lines.push "      const _r = ruleActions(#{rule.id}, _vals_, [], {});"
      lines.push "      left = _r != null ? withLoc(_r, l) : left;"
      lines.push "      continue;"
      lines.push "    }"

    # Ternary operators
    if @_ternaryOps.length
      lines.push ""
      lines.push "    // Ternary operators (derived from Operation rules)"
      for {token: tok, separator, bp, assoc, rule} in @_ternaryOps
        bpExpr = "(BP[#{JSON.stringify tok}] || 0)"
        lines.push "    if (token === #{JSON.stringify tok} && #{bpExpr} >= minBP) {"
        lines.push "      advance();"
        lines.push "      const middle = parseExpression(0);"
        lines.push "      expect(#{JSON.stringify separator});"
        lines.push "      const right = parseExpression(0);"
        lines.push "      const _vals_ = [left, #{JSON.stringify tok}, middle, #{JSON.stringify separator}, right];"
        lines.push "      const $ = _vals_, $0 = 4;"
        lines.push "      const _r = ruleActions(#{rule.id}, _vals_, [], {});"
        lines.push "      left = _r != null ? withLoc(_r, l) : left;"
        lines.push "      continue;"
        lines.push "    }"

    # Scan Expression alternatives for postfix patterns:
    # Rules like "Expression TOKEN Expression" or "Statement TOKEN Expression"
    # where TOKEN has operator precedence (postfix if, postfix unless, etc.)
    # and rules where Expression appears first (postfix for, postfix while)
    if @_exprNT
      postfixTokensEmitted = new Set
      for exprRule in @types[@_exprNT].rules when exprRule.symbols.length is 1
        altName = exprRule.symbols[0]
        altType = @types[altName]
        continue unless altType
        continue if altName is @_operationNT or altName is @_valueNT or altName is @_codeNT

        for rule in altType.rules
          syms = rule.symbols
          continue unless syms.length >= 2

          # Pattern: Expression/Statement POSTFIX_TOKEN NT — postfix operator (3+ symbols)
          # Only match when the left operand is Expression or Statement (not structural NTs like IfBlock)
          if syms.length >= 3 and @types[syms[0]] and not @types[syms[1]] and @types[syms[2]] and (syms[0] is @_exprNT or @_exprStatementAlts?.has(syms[0]))
            postToken = syms[1]
            continue if postfixTokensEmitted.has postToken
            # Skip if this token is a prefix starter — those get full handlers below (parsePostfixFor etc.)
            continue if @_prefixStarters.some((p) -> p.token is postToken)
            postfixTokensEmitted.add postToken
            bp = (@operators[postToken]?.precedence or 0) * 2

            # Collect ALL rules for this postfix token (including longer variants like ELSE continuation)
            allPostRules = []
            for r2 in altType.rules
              s2 = r2.symbols
              if s2.length >= 3 and s2[1] is postToken and @types[s2[0]] and (s2[0] is @_exprNT or @_exprStatementAlts?.has(s2[0]))
                allPostRules.push r2

            # Sort by length: shortest first (base), then longer variants
            allPostRules.sort (a, b) -> a.symbols.length - b.symbols.length
            baseRule = allPostRules[0]
            longerRules = (r for r in allPostRules when r.symbols.length > baseRule.symbols.length)

            lines.push ""
            lines.push "    // Postfix #{postToken} (derived from #{altName} rules)"
            lines.push "    if (token === #{JSON.stringify postToken} && #{bp} >= minBP) {"
            lines.push "      const _pvals_ = [left];"
            lines.push "      _pvals_.push(expect(#{JSON.stringify postToken}));"
            for sym in baseRule.symbols[2..]
              if @types[sym] then lines.push "      _pvals_.push(parse#{sym}());"
              else lines.push "      _pvals_.push(expect(#{JSON.stringify sym}));"

            # Check for longer continuation variants (e.g., POST_IF Expr ELSE INDENT Expr OUTDENT)
            if longerRules.length
              contRule = longerRules[0]
              contStart = baseRule.symbols.length
              contSym = contRule.symbols[contStart]
              if contSym and not @types[contSym]
                lines.push "      if (token === #{JSON.stringify contSym}) {"
                lines.push "        _pvals_.push(expect(#{JSON.stringify contSym}));"
                for sym in contRule.symbols[contStart + 1..]
                  if @types[sym] then lines.push "        _pvals_.push(parse#{sym}());"
                  else lines.push "        _pvals_.push(expect(#{JSON.stringify sym}));"
                lines.push "        const $ = _pvals_, $0 = _pvals_.length - 1;"
                lines.push "        const _r = ruleActions(#{contRule.id}, _pvals_, [], {});"
                lines.push "        left = _r != null ? withLoc(_r, l) : left; continue;"
                lines.push "      }"

            lines.push "      const $ = _pvals_, $0 = _pvals_.length - 1;"
            lines.push "      const _r = ruleActions(#{baseRule.id}, _pvals_, [], {});"
            lines.push "      left = _r != null ? withLoc(_r, l) : left;"
            lines.push "      continue;"
            lines.push "    }"

          # Pattern: Expression KEYWORD — postfix comprehension/loop (terminal second symbol)
          if syms[0] is @_exprNT and not @types[syms[1]] and @_prefixStarters.some((p) -> p.token is syms[1])
            postToken = syms[1]
            continue if postfixTokensEmitted.has "postfix:#{postToken}"
            postfixTokensEmitted.add "postfix:#{postToken}"
            lines.push ""
            lines.push "    // Postfix #{postToken} (derived from #{altName} rules)"
            lines.push "    if (token === #{JSON.stringify postToken} && minBP === 0) {"
            lines.push "      left = parsePostfixFor(left, l);"
            lines.push "      continue;"
            lines.push "    }"

          # Pattern: Expression NONTERMINAL — postfix with nonterminal (e.g., Expression WhileSource)
          if syms.length is 2 and syms[0] is @_exprNT and @types[syms[1]]
            ntSym = syms[1]
            ntFirsts = @firsts[ntSym]
            continue unless ntFirsts?.size
            ntFirsts.forEach (postToken) =>
              return if postfixTokensEmitted.has "postfix:#{postToken}"
              postfixTokensEmitted.add "postfix:#{postToken}"
              lines.push ""
              lines.push "    // Postfix #{postToken} (derived from #{altName} via #{ntSym})"
              lines.push "    if (token === #{JSON.stringify postToken} && minBP === 0) {"
              lines.push "      const _ws = parse#{ntSym}();"
              lines.push "      const _vals_ = [left, _ws];"
              lines.push "      const $ = _vals_, $0 = 1;"
              lines.push "      const _r = ruleActions(#{rule.id}, _vals_, [], {});"
              lines.push "      left = _r != null ? withLoc(_r, l) : left;"
              lines.push "      continue;"
              lines.push "    }"

    lines.push ""
    lines.push "    break;"
    lines.push "  }"
    lines.push "  return left;"
    lines.push "}"
    lines.join '\n'

  # --- Specialized nonterminal parsers ---
  Generator::_generateRDSpecialized = (name) ->
    switch name
      when 'For'       then @_generateRDFor()
      when 'Object'    then @_generateRDObject()
      when 'AssignObj' then @_generateRDAssignObj()
      else throw new Error "No specialized generator for #{name}"

  Generator::_generateRDFor = ->
    """
    function parseFor() {
      const l = loc();
      expect('FOR');
      // Check for range: FOR Range Block (use speculation — [a,b] could be destructuring)
      if (token === '[') {
        const range = speculate(() => parseRange());
        if (range !== null) {
          if (token === 'BY') {
            advance();
            const step = parseExpression(0);
            const block = parseBlock();
            return withLoc(["for-in", [], range, step, null, block], l);
          }
          const block = parseBlock();
          return withLoc(["for-in", [], range, null, null, block], l);
        }
      }
      // Check for AWAIT (async for-as)
      if (token === 'AWAIT') {
        advance();
        const vars = parseForVariables();
        expect('FORAS');
        const iter = parseExpression(0);
        let guard = null;
        if (token === 'WHEN') { advance(); guard = parseExpression(0); }
        const block = parseBlock();
        return withLoc(["for-as", vars, iter, true, guard, block], l);
      }
      // Check for OWN (for own k of obj)
      if (token === 'OWN') {
        advance();
        const vars = parseForVariables();
        expect('FOROF');
        const obj = parseExpression(0);
        let guard = null;
        if (token === 'WHEN') { advance(); guard = parseExpression(0); }
        const block = parseBlock();
        return withLoc(["for-of", vars, obj, true, guard, block], l);
      }
      // Regular for: parse variables, then dispatch on FORIN/FOROF/FORAS
      const vars = parseForVariables();
      if (token === 'FORIN') {
        advance();
        const arr = parseExpression(0);
        let step = null, guard = null;
        if (token === 'WHEN') { advance(); guard = parseExpression(0); }
        if (token === 'BY') { advance(); step = parseExpression(0); }
        if (!guard && token === 'WHEN') { advance(); guard = parseExpression(0); }
        const block = parseBlock();
        return withLoc(["for-in", vars, arr, step, guard, block], l);
      }
      if (token === 'FOROF') {
        advance();
        const obj = parseExpression(0);
        let guard = null;
        if (token === 'WHEN') { advance(); guard = parseExpression(0); }
        const block = parseBlock();
        return withLoc(["for-of", vars, obj, false, guard, block], l);
      }
      if (token === 'FORAS' || token === 'FORASAWAIT') {
        const isAsync = token === 'FORASAWAIT';
        advance();
        const iter = parseExpression(0);
        let guard = null;
        if (token === 'WHEN') { advance(); guard = parseExpression(0); }
        const block = parseBlock();
        return withLoc(["for-as", vars, iter, isAsync, guard, block], l);
      }
      throw new Error('Parse error in For: expected FORIN/FOROF/FORAS after variables, got ' + token);
    }
    """

  Generator::_generateRDAssignObj = ->
    """
    function parseAssignObj() {
      const l = loc();
      // Rest: {...x}
      if (token === '...') return parseObjRestValue();
      // Parse the key — try ObjAssignable first (broader), fall back to SimpleObjAssignable
      const key = parseObjAssignable();
      // Property: key: value
      if (token === ':') {
        advance();
        let value;
        if (token === 'INDENT') { advance(); value = parseExpression(0); expect('OUTDENT'); }
        else { value = parseExpression(0); }
        return [key, value, ":"];
      }
      // Default: key = value (only valid for SimpleObjAssignable keys)
      if (token === '=') {
        advance();
        let value;
        if (token === 'INDENT') { advance(); value = parseExpression(0); expect('OUTDENT'); }
        else { value = parseExpression(0); }
        return [key, value, "="];
      }
      // Shorthand: {x}
      return [key, key, null];
    }
    """

  Generator::_generateRDObject = ->
    """
    function parseObject() {
      const l = loc();
      expect('{');
      // Empty object
      if (token === '}') {
        advance();
        return withLoc(["object"], l);
      }
      // Parse assign list (handles key: value pairs)
      const list = parseAssignList();
      match(',');
      // Check for comprehension (FOR after the first key:value)
      if (token === 'FOR') {
        // Object comprehension: {k: v for ...}
        // list should have [key, value, ":"] as first entry
        const first = list[0];
        const key = first ? first[0] : list[0];
        const val = first ? first[1] : list[1];
        advance(); // consume FOR
        if (token === 'OWN') {
          advance();
          const vars = parseForVariables();
          expect('FOROF');
          const obj = parseExpression(0);
          let guard = null;
          if (token === 'WHEN') { advance(); guard = parseExpression(0); }
          match(',');
          expect('}');
          return withLoc(["object-comprehension", key, val, [["for-of", vars, obj, true]], guard ? [guard] : []], l);
        }
        const vars = parseForVariables();
        if (token === 'FOROF') {
          advance();
          const obj = parseExpression(0);
          let guard = null;
          if (token === 'WHEN') { advance(); guard = parseExpression(0); }
          match(',');
          expect('}');
          return withLoc(["object-comprehension", key, val, [["for-of", vars, obj, false]], guard ? [guard] : []], l);
        }
        throw new Error('Parse error in Object comprehension: unexpected ' + token);
      }
      expect('}');
      return withLoc(["object", ...list], l);
    }
    """

  # --- Unary prefix parser (GENERATED from grammar rules) ---
  Generator::_generateRDUnaryGeneric = ->
    lines = []
    lines.push "function parseUnary() {"
    lines.push "  const l = loc();"

    # Prefix operators (from Operation rules)
    lines.push "  // Prefix operators (derived from Operation rules)"
    seen = new Set
    for {token: tok, bp, rule} in @_prefixOps
      continue if seen.has tok
      seen.add tok
      syms = rule.symbols
      isCategoryToken = tok is tok.toUpperCase() and tok.length > 2
      lines.push "  if (token === #{JSON.stringify tok}) {"
      if isCategoryToken
        lines.push "    const _op = tokenText; advance();"
      else
        lines.push "    advance();"
      rightNT = syms[1]
      bpStr = "#{bp * 2}"  # Use the rule's actual binding power (with prec override)
      # If the right side is Expression, pass binding power
      if rightNT is @_exprNT or rightNT is @_operationNT
        lines.push "    const expr = parseExpression(#{bpStr});"
      else
        lines.push "    const expr = parse#{rightNT}();"
      lines.push "    const _vals_ = [#{if isCategoryToken then '_op' else JSON.stringify tok}, expr];"
      lines.push "    const $ = _vals_, $0 = 1;"
      lines.push "    const _r = ruleActions(#{rule.id}, _vals_, [], {});"
      lines.push "    return _r != null ? withLoc(_r, l) : expr;"
      lines.push "  }"

    # Atom tokens (from Value/Literal chain)
    lines.push "  // Atoms (derived from Value/Literal rules)"
    atomSeen = new Set

    # Detect Range nonterminal for Range/Array speculation
    rangeNT = null
    if @_valueNT
      for rule in @types[@_valueNT].rules when rule.symbols.length is 1
        ntType = @types[rule.symbols[0]]
        if ntType?.rules.some((r) => r.symbols[0] is '[' and r.symbols.length >= 5)
          rangeNT = rule.symbols[0]
          break

    # Range/Array ambiguity: both start with '['. Use speculation for Range first.
    if rangeNT
      atomSeen.add '['
      lines.push "  if (token === '[') {"
      lines.push "    const _range = speculate(() => parse#{rangeNT}());"
      lines.push "    if (_range !== null) return _range;"
      lines.push "    return parseArray();"
      lines.push "  }"

    # Handle tokens that appear in multiple Value alternatives with lookahead
    # @ → This (bare @) or ThisProperty (@ Property) — check if PROPERTY follows
    lines.push "  if (token === '@') {"
    lines.push "    const _saved = mark(); advance();"
    lines.push "    if (token === 'PROPERTY') { reset(_saved); return parseThisProperty(); }"
    lines.push "    reset(_saved); return parseThis();"
    lines.push "  }"
    atomSeen.add '@'
    # SUPER → Super (SUPER . Property | SUPER INDEX_START) or Invocation (SUPER Arguments)
    lines.push "  if (token === 'SUPER') {"
    lines.push "    const _saved = mark(); advance();"
    lines.push "    if (token === '.' || token === 'INDEX_START') { reset(_saved); return parseSuper(); }"
    lines.push "    reset(_saved); return parseInvocation();"
    lines.push "  }"
    atomSeen.add 'SUPER'

    for {token: tok, rule, nonterminal} in @_atomTokens
      continue if atomSeen.has tok
      atomSeen.add tok
      if rule.symbols.length is 1 and not @types[rule.symbols[0]]
        lines.push "  if (token === #{JSON.stringify tok}) {"
        lines.push "    const _vals_ = [tokenText]; advance();"
        lines.push "    const $ = _vals_, $0 = 0;"
        lines.push "    const _r = ruleActions(#{rule.id}, _vals_, [], {});"
        lines.push "    return _r != null ? _r : _vals_[0];"
        lines.push "  }"
      else if nonterminal
        lines.push "  if (token === #{JSON.stringify tok}) return parse#{nonterminal}();"

    # Structural atoms — any Value alternative not yet covered
    if @_valueNT
      for rule in @types[@_valueNT].rules when rule.symbols.length is 1
        ntName = rule.symbols[0]
        continue unless @types[ntName]
        firsts = @firsts[ntName]
        continue unless firsts
        firsts.forEach (tok) =>
          unless atomSeen.has tok
            atomSeen.add tok
            lines.push "  if (token === #{JSON.stringify tok}) return parse#{ntName}();"
    # Code (arrow functions) as atoms
    if @_codeNT and @firsts[@_codeNT]
      @firsts[@_codeNT].forEach (tok) =>
        unless atomSeen.has tok
          atomSeen.add tok
          lines.push "  if (token === #{JSON.stringify tok}) return parse#{@_codeNT}();"

    lines.push "  if (token === '...') return parseSplat();" unless atomSeen.has '...'
    # Statement tokens as atoms — allows 'break if done', 'return x unless err' patterns
    lines.push "  if (token === 'STATEMENT') { const v = tokenText; advance(); return v; }"
    lines.push "  if (token === 'RETURN') return parseReturn();"
    # Fire-and-forget effect (~> expr) — prefix form without left-hand side
    lines.push "  if (token === 'REACT_ASSIGN') return parseReactAssign();"
    lines.push "  throw new Error('Parse error: unexpected token ' + token + ' at line ' + ((tokenLoc && tokenLoc.r || 0) + 1));"
    lines.push "}"
    lines.join '\n'

  # --- Postfix for (comprehensions) — kept as template, dispatched from grammar ---
  Generator::_generateRDPostfixForGeneric = ->
    # Check if any Expression alternative has postfix comprehension rules
    # (rules where Expression appears first, indicating postfix forms)
    return '' unless @_exprNT
    hasPostfix = false
    for rule in @types[@_exprNT].rules when rule.symbols.length is 1
      altType = @types[rule.symbols[0]]
      continue unless altType
      if altType.rules.some((r) => r.symbols[0] is @_exprNT and r.symbols.length >= 3)
        hasPostfix = true
        break
    return '' unless hasPostfix

    # Generate postfix for by examining the postfix For rules
    # These follow patterns: Expression FOR ForVariables FORIN/FOROF/FORAS Expression ...
    """
    function parsePostfixFor(expr, l) {
      advance(); // consume FOR
      if (token === 'AWAIT') {
        advance();
        const vars = parseForVariables();
        if (token === 'FORAS') {
          advance();
          const iter = parseExpression(0);
          const guard = token === 'WHEN' ? (advance(), parseExpression(0)) : null;
          return withLoc(["comprehension", expr, [["for-as", vars, iter, true, null]], guard ? [guard] : []], l);
        }
      }
      if (token === 'OWN') {
        advance();
        const vars = parseForVariables();
        expect('FOROF');
        const obj = parseExpression(0);
        const guard = token === 'WHEN' ? (advance(), parseExpression(0)) : null;
        return withLoc(["comprehension", expr, [["for-of", vars, obj, true]], guard ? [guard] : []], l);
      }
      if (token === '[') {
        const range = speculate(() => parseRange());
        if (range !== null) {
          if (token === 'BY') { advance(); const step = parseExpression(0); return withLoc(["comprehension", expr, [["for-in", [], range, step]], []], l); }
          return withLoc(["comprehension", expr, [["for-in", [], range, null]], []], l);
        }
      }
      const vars = parseForVariables();
      if (token === 'FORIN') {
        advance(); const arr = parseExpression(0);
        let step = null, guard = null;
        if (token === 'WHEN') { advance(); guard = parseExpression(0); }
        if (token === 'BY') { advance(); step = parseExpression(0); }
        if (!guard && token === 'WHEN') { advance(); guard = parseExpression(0); }
        return withLoc(["comprehension", expr, [["for-in", vars, arr, step]], guard ? [guard] : []], l);
      }
      if (token === 'FOROF') {
        advance(); const obj = parseExpression(0);
        const guard = token === 'WHEN' ? (advance(), parseExpression(0)) : null;
        return withLoc(["comprehension", expr, [["for-of", vars, obj, false]], guard ? [guard] : []], l);
      }
      if (token === 'FORAS' || token === 'FORASAWAIT') {
        const isAsync = token === 'FORASAWAIT'; advance();
        const iter = parseExpression(0);
        const guard = token === 'WHEN' ? (advance(), parseExpression(0)) : null;
        return withLoc(["comprehension", expr, [["for-as", vars, iter, isAsync, null]], guard ? [guard] : []], l);
      }
      throw new Error('Parse error in postfix for: unexpected ' + token);
    }
    """

  # --- ExpressionLine parser (generic) ---
  Generator::_generateRDExpressionLine = ->
    return '' unless @_exprLineNT
    type = @types[@_exprLineNT]
    return '' unless type
    # Generate as a choice dispatch
    @_generateRDChoice @_exprLineNT

  # --- Operation parser (just delegates to expression with BP) ---
  Generator::_generateRDOperation = ->
    """
    function parseOperation() {
      return parseExpression(0);
    }
    """

  # --- OperationLine parser (generic) ---
  Generator::_generateRDOperationLine = ->
    return '' unless @_operationLineNT
    # Generate using the generic keyword/sequence generator
    @_generateRDGeneric @_operationLineNT

  # --- Generic fallback nonterminal parser ---
  Generator::_generateRDGeneric = (name) ->
    fnName = "parse#{name}"
    type = @types[name]
    rules = type.rules

    return @_generateRDToken name if rules.length is 1 and rules[0].symbols.length is 1 and not @types[rules[0].symbols[0]]
    return @_generateRDChoice name if rules.every (r) -> r.symbols.length is 1

    lines = []
    lines.push "function #{fnName}() {"
    lines.push "  const l = loc();"

    if rules.length is 1
      lines.push @_generateRDRuleBody rules[0], '  '
    else
      # Try to dispatch on first token
      firstKey = true
      defaultRule = null
      for rule in rules
        first = rule.symbols[0]
        if first is '' or (first and @types[first])
          defaultRule ?= rule
          continue
        prefix = if firstKey then "if" else "else if"
        firstKey = false
        lines.push "  #{prefix} (token === #{JSON.stringify first}) {"
        lines.push @_generateRDRuleBody rule, '    '
        lines.push "  }"

      if defaultRule
        if firstKey
          lines.push @_generateRDRuleBody defaultRule, '  '
        else
          lines.push "  else {"
          lines.push @_generateRDRuleBody defaultRule, '    '
          lines.push "  }"
      else unless firstKey
        lines.push "  else {"
        lines.push "    throw new Error('Parse error in #{name}: unexpected ' + token);"
        lines.push "  }"

    lines.push "}"
    lines.join '\n'
